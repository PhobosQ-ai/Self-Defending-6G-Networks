<h1>Self-Defending 6G Networks Through AI-Driven Adaptive Decoy Generation</h1>
<p>This repository contains the simulation code for the research paper titled &quot;Self-Defending 6G Networks Through AI-Driven Adaptive Decoy Generation at the Edge&quot;.</p>
<h2>Overview</h2>
<p>The framework provides a proactive security mechanism for 6G networks using three core AI components:</p>
<ol>
<li><strong>Conditional GAN (cGAN)</strong>: To generate realistic, context-aware network decoys.</li>
<li><strong>Reinforcement Learning (PPO)</strong>: To dynamically manage decoy deployment strategies.</li>
<li><strong>Federated Learning</strong>: To synchronize models across distributed edge nodes in a privacy-preserving manner.</li>
</ol>
<h2>Features</h2>
<ul>
<li>Simulation of edge nodes in a 6G network environment.</li>
<li>Generation of adaptive decoys using conditional GANs.</li>
<li>Dynamic decision-making with Proximal Policy Optimization (PPO).</li>
<li>Privacy-preserving model synchronization via Federated Learning.</li>
<li>Data preprocessing and visualization for the CIC-IoT-2023 dataset.</li>
</ul>
<h2>Installation</h2>
<ol>
<li>Clone the repository:<pre><code class="language-bash">git clone &lt;repository-url&gt;
cd py-files
</code></pre>
</li>
</ol>
<h1>Self-Defending 6G Networks — Adaptive Decoy Generation (Math-enabled README)</h1>
<p>This folder contains the simulation code that implements the Adaptive Decoy Generation framework described in the paper &quot;Self-Defending 6G Networks Through AI-Driven Adaptive Decoy Generation at the Edge&quot;.</p>
<p>This README includes the core mathematical expressions used by the implementation so they render in Markdown viewers that support TeX (MathJax/KaTeX).</p>
<h2>Quick start</h2>
<ol>
<li>Clone the repository and change into the python folder:</li>
</ol>
<pre><code class="language-bash">git clone &lt;repository-url&gt;
cd &quot;c:\PhobosQ - docs\latex docs\6g files\py files&quot;
</code></pre>
<ol start="2">
<li>Install dependencies:</li>
</ol>
<pre><code class="language-bash">pip install -r requirements.txt
</code></pre>
<ol start="3">
<li>Run the simulation:</li>
</ol>
<pre><code class="language-bash">python main.py
</code></pre>
<h2>What this code implements</h2>
<ul>
<li>Conditional Generative Adversarial Network (cGAN) for context-aware decoy generation.</li>
<li>Reinforcement Learning (PPO) agent for dynamic decoy deployment and resource management.</li>
<li>Federated Learning (FedAvg-like) synchronization across distributed edge nodes.</li>
<li>Data loading, preprocessing, plotting utilities and a simulation harness.</li>
</ul>
<h2>Mathematical details (rendered in TeX-aware Markdown)</h2>
<p>Below are the key equations and loss functions referenced in the implementation and paper. Put simply, these are the formal contracts that guide training and synchronization.</p>
<h3>1) cGAN objective</h3>
<p>The canonical conditional GAN objective used to train generator G and discriminator D (conditioned on y) is:</p>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0A%5Cmin_G%20%5Cmax_D%20%5Cmathbb%7BE%7D_%7Bx%5Csim%20p_%7B%5Ctext%7Bdata%7D%7D%2C%20y%5Csim%20p_y%7D%20%5B%5Clog%20D(x%2Cy)%5D%20%2B%20%5Cmathbb%7BE%7D_%7Bz%5Csim%20p_z%2C%20y%5Csim%20p_y%7D%20%5B%5Clog%20(1%20-%20D(G(z%2Cy)%2Cy))%5D%0A" alt="
\min_G \max_D \mathbb{E}_{x\sim p_{\text{data}}, y\sim p_y} [\log D(x,y)] + \mathbb{E}_{z\sim p_z, y\sim p_y} [\log (1 - D(G(z,y),y))]
" /></p>
<p>In practice we use a stabilized variant (WGAN-GP) for improved training stability.</p>
<h3>2) WGAN-GP discriminator loss (used for stability)</h3>
<p>Using the Wasserstein loss with gradient penalty (coefficient $\lambda$):</p>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0AL_D%20%3D%20%5Cmathbb%7BE%7D_%7B%5Ctilde%7Bx%7D%20%5Csim%20p_g%7D%20%5BD(%5Ctilde%7Bx%7D%2Cy)%5D%20-%20%5Cmathbb%7BE%7D_%7Bx%20%5Csim%20p_%7B%5Ctext%7Bdata%7D%7D%7D%20%5BD(x%2Cy)%5D%20%2B%20%5Clambda%20%5Cmathbb%7BE%7D_%7B%5Chat%7Bx%7D%20%5Csim%20p_%7B%5Chat%7Bx%7D%7D%7D%20%5Cbig%5B%20(%5C%7C%5Cnabla_%7B%5Chat%7Bx%7D%7D%20D(%5Chat%7Bx%7D%2Cy)%5C%7C_2%20-%201)%5E2%20%5Cbig%5D%0A" alt="
L_D = \mathbb{E}_{\tilde{x} \sim p_g} [D(\tilde{x},y)] - \mathbb{E}_{x \sim p_{\text{data}}} [D(x,y)] + \lambda \mathbb{E}_{\hat{x} \sim p_{\hat{x}}} \big[ (\|\nabla_{\hat{x}} D(\hat{x},y)\|_2 - 1)^2 \big]
" /></p>
<p>where $\hat{x}$ is sampled uniformly on straight lines between real and generated samples.</p>
<p>Generator loss (Wasserstein style) typically minimizes the negative critic score:</p>
<p><img src="https://i.upmath.me/svg/%0AL_G%20%3D%20-%5Cmathbb%7BE%7D_%7Bz%5Csim%20p_z%7D%5BD(G(z%2Cy)%2Cy)%5D%0A" alt="
L_G = -\mathbb{E}_{z\sim p_z}[D(G(z,y),y)]
" />
$$</p>
<h3>3) Adversarial feedback (attack-driven fine-tuning)</h3>
<p>When attack interaction data $\mathcal{D}_{\text{attack}}$ is available, the generator fine-tuning loss term used in the paper is:</p>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0A%5Cmathcal%7BL%7D_%7B%5Ctext%7Battack%7D%7D%20%3D%20%5Cmathbb%7BE%7D_%7Bx_%7B%5Ctext%7Battack%7D%7D%5Csim%5Cmathcal%7BD%7D_%7B%5Ctext%7Battack%7D%7D%7D%20%5CBig%5B%20%5Cmin_%7Bz%7D%20%5C%7C%20G(z%2C%20y_%7B%5Ctext%7Bcontext%7D%7D)%20-%20x_%7B%5Ctext%7Battack%7D%7D%20%5C%7C_2%5E2%20%5CBig%5D%0A" alt="
\mathcal{L}_{\text{attack}} = \mathbb{E}_{x_{\text{attack}}\sim\mathcal{D}_{\text{attack}}} \Big[ \min_{z} \| G(z, y_{\text{context}}) - x_{\text{attack}} \|_2^2 \Big]
" /></p>
<p>This expresses a GAN inversion (finding latent $z$ that reconstructs attacker-observed samples) or training an encoder to map attacks into the latent space for targeted fine-tuning.</p>
<h3>4) Reinforcement learning objective (policy optimization)</h3>
<p>The agent seeks a policy $\pi$ that maximizes the expected discounted return:</p>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0AJ(%5Cpi)%20%3D%20%5Cmathbb%7BE%7D%5Cleft%5B%5Csum_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%20%5Cgamma%5Et%20R(s_t%2Ca_t)%5Cright%5D%0A" alt="
J(\pi) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t R(s_t,a_t)\right]
" /></p>
<p>For PPO (Proximal Policy Optimization) the clipped surrogate objective is commonly used:</p>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0AL%5E%7B%5Ctext%7BCLIP%7D%7D(%5Ctheta)%20%3D%20%5Cmathbb%7BE%7D_t%20%5Cbig%5B%20%5Cmin%5Cbig(%20r_t(%5Ctheta)%5Chat%7BA%7D_t%2C%20%5C%3B%20%5Cmathrm%7Bclip%7D(r_t(%5Ctheta)%2C1-%5Cepsilon%2C1%2B%5Cepsilon)%5Chat%7BA%7D_t%20%5Cbig)%20%5Cbig%5D%0A" alt="
L^{\text{CLIP}}(\theta) = \mathbb{E}_t \big[ \min\big( r_t(\theta)\hat{A}_t, \; \mathrm{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A}_t \big) \big]
" /></p>
<p>where</p>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0Ar_t(%5Ctheta)%20%3D%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_t%7Cs_t)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(a_t%7Cs_t)%7D%2C%20%5Cquad%20%5Chat%7BA%7D_t%20%3D%20%5Ctext%7Badvantage%20estimate%7D%2C%20%5Cquad%20%5Cepsilon%5Ctext%7B%20is%20clip%20parameter.%7D%0A" alt="
r_t(\theta) = \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}, \quad \hat{A}_t = \text{advantage estimate}, \quad \epsilon\text{ is clip parameter.}
" /></p>
<h3>5) Federated averaging (synchronization)</h3>
<p>The federated aggregation step used by the server in the paper is expressed as (a FedAvg-like weighted update):</p>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0A%09heta_%7B%5Ctext%7Bglobal%7D%7D%5E%7B(t%2B1)%7D%20%5Cleftarrow%20%5Ctheta_%7B%5Ctext%7Bglobal%7D%7D%5E%7B(t)%7D%20-%20%5Ceta%20%5Csum_%7Bk%5Cin%20S_t%7D%20%5Cfrac%7Bn_k%7D%7Bn%7D%20%5CDelta%5Ctheta_k%5E%7B(t)%7D%0A" alt="
	heta_{\text{global}}^{(t+1)} \leftarrow \theta_{\text{global}}^{(t)} - \eta \sum_{k\in S_t} \frac{n_k}{n} \Delta\theta_k^{(t)}
" /></p>
<p>where $n_k$ is the size of node $k$'s local dataset, $n=\sum_k n_k$, and $\Delta\theta_k^{(t)}$ denotes the local model update returned by node $k$.</p>
<h3>6) Evaluation metrics (detection rates)</h3>
<p>Detection Rate (DR):</p>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0A%5Cmathrm%7BDR%7D%20%3D%20%5Cfrac%7B%5Ctext%7BNumber%20of%20detected%20attacks%7D%7D%7B%5Ctext%7BTotal%20number%20of%20attacks%7D%7D%20%5Ctimes%20100%5C%25.%0A" alt="
\mathrm{DR} = \frac{\text{Number of detected attacks}}{\text{Total number of attacks}} \times 100\%.
" /></p>
<p>False Positive Rate (FPR):</p>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0A%5Cmathrm%7BFPR%7D%20%3D%20%5Cfrac%7B%5Ctext%7BNumber%20of%20false%20positives%7D%7D%7B%5Ctext%7BTotal%20legitimate%20interactions%7D%7D%20%5Ctimes%20100%5C%25.%0A" alt="
\mathrm{FPR} = \frac{\text{Number of false positives}}{\text{Total legitimate interactions}} \times 100\%.
" /></p>
<p>Latency (interaction-to-alert) is measured as the elapsed time between the first packet interacting with a decoy and the generated alert at the edge node.</p>
<h2>Project structure</h2>
<ul>
<li><code>main.py</code> — simulation harness and orchestrator.</li>
<li><code>gan_model.py</code> — cGAN (Generator / Discriminator) implementations.</li>
<li><code>rl_agent.py</code> — PPO agent and utilities.</li>
<li><code>edge_node.py</code> — simulated edge node behaviors and decoy deployment.</li>
<li><code>data_loader.py</code> — dataset loading and preprocessing (CIC-IoT-2023 adapter).</li>
<li><code>federated_learning.py</code> — simple FedAvg orchestration used in the simulation.</li>
<li><code>plots.py</code> — helper functions to render figures used in the paper.</li>
<li><code>requirements.txt</code> — Python dependencies.</li>
</ul>